# PowerSense â€” Default Hyperparameters
# Matches all values reported in the paper (Section V-C)

benchmark:
  total_slots: 5000
  transformers: 8
  panels: 48
  circuits: 384
  facility_capacity_kw: 4500
  product_types: 12
  power_range_kw: [0.5, 1.2]
  test_duration_hours: [2, 8]
  phases_per_product: [3, 6]
  peak_to_avg_ratio: [1.4, 2.1]
  production_rate_per_day: [800, 1200]
  cv_range: [0.05, 0.15]          # sigma/mu
  decision_epoch_minutes: 5
  simulation_days: 30

gat:
  layers: 3
  hidden_dim: 64
  attention_heads: 4
  activation: elu
  optimizer: adam
  learning_rate: 0.001
  training_snapshots: 100000

tft:
  attention_heads: 4
  hidden_size: 128
  lstm_layers: 2
  forecast_horizons: [5, 15, 60, 240, 1440]  # minutes
  quantiles: [0.1, 0.5, 0.9]
  training_months: 6
  train_val_test_split: [0.8, 0.1, 0.1]

ppo:
  mlp_layers: 3
  hidden_units: 256
  learning_rate: 0.0003
  batch_size: 2048
  clip_ratio: 0.2
  discount_factor: 0.99
  total_steps: 5000000
  convergence_steps: 3500000
  penalty_lambda_start: 10
  penalty_lambda_end: 100
  safety_margin_gamma: 0.1

infrastructure:
  gpu: "NVIDIA A100"
  pytorch_version: "2.1"
  pyg_version: "2.4"
  approx_training_hours: 34
